{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034f4d59",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) with LangChain\n",
    "- PDF 문서를 로드하고 검색 기반 질의응답 시스템 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e5663",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 (PDFLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8306f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf langchain-community langchain-text-splitters langchain-openai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b21b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = \"./income_tax.pdf\" # PDF 파일 경로 설정\n",
    "loader = PyPDFLoader(file_path) # 경로의 PDF 파일을 PyPDFLoader로 로드\n",
    "pages = [] # 추출된 문서를 저장할 리스트를 초기화\n",
    "\n",
    "# PDF에서 텍스트 데이터를 추출하여 텍스트 데이터로 반환\n",
    "text_content = loader.load() \n",
    "\n",
    "# PDF의 각 페이지를 하나씩 가져와 docs에 저장\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "\n",
    "# pages\n",
    "# pages[36]\n",
    "    \n",
    "# print(pages[0].page_content[:100]) # 첫 번째 페이지의 처음 100자 출력\n",
    "# print(pages[0].metadata) # 첫 번째 문서(첫 페이지)의 메타데이터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd0d9f",
   "metadata": {},
   "source": [
    "### py-zerox : 상용 AI API 끌어다 쓰기 가능\n",
    "- https://pypi.org/project/py-zerox/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f28171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q py-zerox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1636579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement poppler (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for poppler\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4024231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea9db7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c198be62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"OpenAI API Key:\", os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c321c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f7ac56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-BYJWDV1v0Jk8yR7T7WmdkcMX on tokens per min (TPM): Limit 200000, Used 200000, Requested 807. Please try again in 242ms. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "ZeroxOutput(completion_time=79526.772, file_name='income_tax', input_tokens=590032, output_tokens=16783, pages=[Page(content='# 소득세법\\n\\n[시행 2026. 1. 23.] [법률 제21065호, 2025. 10. 1., 타법개정]  \\n기획재정부 (세제실세정과) 044-215-4312  \\n기획재정부 (소득세제과) 044-215-4216  \\n기획재정부 (음식세제과) 044-215-4233  \\n기획재정부 (소득세과(사연순, 기타소득)) 044-215-4217  \\n\\n---\\n\\n## 제1장 총칙\\n\\n### 제조(목적)\\n\\n이 법은 개인의 소득에 대하여 소득의 성격과 납세자의 부담능력 등에 따라 적정하게 과세함으로써 조세부담의 형평을 도모하고 재정수입의 원활한 조달에 이바지함을 목적으로 한다.  \\n[본조신설 2009. 12. 31.]  \\n[종전 제1조는 제2조로 이동 <2009. 12. 31.>]\\n\\n### 제2조(정의)\\n\\n① 이 법에서 사용하는 용어의 뜻은 다음과 같다.  \\n1. “거주자”란 국내에 주소를 둔지 183일 이상의 거소(居所)를 두 개인을 말한다.  \\n2. “비거주자”란 거주자가 아닌 자를 말한다.  \\n3. “내국법인”이란 법인세법 제2조제1호에 따른 내국법인을 말한다.  \\n4. “외국법인”이란 법인세법 제2조제3호에 따른 외국법인을 말한다.  \\n5. “사업자”란 사업소득이 있는 거주자를 말한다.  \\n6. 제1항에 따른 소득·자산과 비거주자의 구분은 대통령령으로 정한다.  \\n[본조신설 2009. 12. 31.]\\n\\n### 제2조의2(납세의무)\\n\\n다음 각 호의 이 하나에 해당하는 개인은 이 법에 따라 각자의 소득에 대한 소득세를 납부할 의무를 진다.  \\n1. 거주자  \\n2. 비거주자  \\n3. 내국법인  \\n4. 외국법인인 국내 영업소(出張所, 그 밖에 이에 준하는 것을 포함한다. 이하 같다)  \\n\\n5. 그 밖에 이 법에 의해 정하는 원천징수의무자  \\n3. “국세기본법 제32조제1항에 따른 법인 이란 단체 같은 제4조에 따라 법인으로 보는 단체”는 “법인으로 보는 단체”란 하다 외의 법인 아닌 단체는 국내에 주사무소 또는 사업의 실제 관리장소를 두는 경우에는 1거주자로 본다.  \\n이 법의 각 단체에 각 구성이 별도의 법인 세로부터 소득세를 법인세로 납부할 구성을 진다.  \\n[제3조 2010. 12. 27, 2013. 1. 1, 2018. 12. 31.]\\n\\n---\\n\\n## 제3장\\n\\n구성이 이익인 경우를 정하여 구성원별의 이익 분배비율 확인이 있는 경우  \\n구성이 이익인 경우를 정하여 각 구성원별의 이익 분배비율이 정해져 있지 아니하나 사실상 구성원별의 이익 분배비율로 다른 부분이 진다.  \\n[본조신설 2018. 12. 31.]', content_length=1245, page=1), Page(content='# 소득세법\\n\\n1. 확인되는 부분: 해당 구성원별로 소득세 또는 법인세에 대한 납세의무 부담  \\n2. 확인되지 않는 부분: 해당 단체를 1개 또는 1개의 갱신으로 보아 소득세에 대한 납세의무 부담  \\n    * 제43조 및 제43조의4는 불공하고 법인이 아닌 단체에 대한 특별히 필요한 투자자권을 하여 모든 금전 등을 가진 재산가치가 있는 투자자산을 취득, 처분하는 것 외의 방법으로 운영하고 그 결과를 투자자에게 배분하기 위하여 민감하게 투자행위를 하는 기구로 국외의 기구를 한다. 이하 감사법 제119조의2제1항제2호에 따라 국내원천소득의 실질귀속자로 보는 그 국외투자기구는 1개 투자자로서 소득세를 납부할 의무 진다. <법전정 2009. 12. 31.>  \\n   [제1조에서 이동, 중점 제2조는 제2조로 이동 <2009. 12. 31.>]\\n\\n## 제22조(간세임무의 법칙)  \\n① 제43조에 따라 공동사업에 관한 소득세액을 계산하는 경우에는 해당 공동사업자별로 납세의무를 진다. 단, 제43조제3항에 따른 주요 공동사업자가 이하에 행사에 포함될 경우 합산납세의 소득금액에 대해서는 주요 공동사업자와 특수관계인 같은 제3항의 단순변별에 해당하는 그의 소득금액을 한도로 주요 공동사업자와 면세의무를 진다. <개정 2012. 1. 1, 2013. 1. 1.>  \\n② 제44조에 따라 피숙의의 소득금액에 대해서 재해하는 경우에는 그 상속인의 면세의무 진다.  \\n   * 제101조제1항의 자산을 직접 양도한 것으로 보는 경우 그 양도소득에 대해서 증여받은 자가 여전히 해당한다. <개정 2020. 12. 29.>  \\n③ 제127조에 따라 원천징수는 소득으로 제14조제1항 또는 그 발행에 따른 제14조제1항에 따른 소득과세에 합산되지 아니하는 소득의 경우에는 해당 자산은 공동으로 소유하는 각 거주자가 소득의무를 진다. <개정 2020. 12. 29.>  \\n\\n## 제23조(신탁재산 귀속 소득에 대한 납세의무의 법칙)  \\n신탁재산에 귀속되는 소득은 그 신탁의 이익을 받을 수익자(수익자가 상여하는 경우에는 그 상속인)에 귀속되는 것으로 본다.  \\n제18항에 불구하고 위탁재산은 신탁재산을 실질적으로 통제하는 등 대동법령으로 정의하는 요건을 충족하는 신탁의 경우에는 그 신탁에 귀속되는 소득은 위탁자에게 귀속되는 것으로 본다. <개정 2023. 12. 31.>  \\n\\n## 제33조(과세소득의 법칙)  \\n거주자에게 이 법에 의하여 정해진 모든 소득에 대해서 과세한다. 단, 해당 과세기류가 중요 연도 10년 부터 국내에 주소가 있는 자가 그의 행편이 틀리 외국인 거주자가 이미 과세되는 소득 국외에서 발생한 소득의 경우 국내에서 지급되거나 국가로 귀속되는 소득에 대해서 과세된다.  \\n비과세소득에 제119조의2에 따라 국내원천소득에 대해서만 과세한다.  \\n제1항 및 제2항을 적용하는 경우 \"조세특례제한법\" 제100조제14항의 동의자에게는 같은 법 제100조제18제1항에 따라 배분받는 소득 및 같은 법 제100조의2제21항에 따라 분배받는 자산 중 분배받는 자산의 최근 분배법의 지분을 초과해서 발생하는 소득에 대하여 과세한다.  \\n[전문개정 2009. 12. 31.]\\n\\n## 제34조(소득의 구분)  \\n거주자의 소득은 다음 각 호와 같이 구분한다.  \\n1. 종합소득  \\n   이 법에 따라 과세되는 모든 소득에서 제2호 및 제3호에 따른 소득으로 제외될 소득으로 다음 각 목의 소득을 포함한 것\\n', content_length=1677, page=2), Page(content='소득세법\\n\\n가. 이자소득  \\n나. 배당소득  \\n다. 사업소득  \\n라. 근로소득  \\n마. 연금소득  \\n바. 기타소득  \\n\\n2. 퇴직소득  \\n2의1. 수정 <2024. 12. 31.>  \\n3. 양도소득  \\n② 제1항에 따른 소득을 구분할 때 다음 각 호의 신탁에 의한 「신탁법」 제2조에 따라 수탁자에게 이전되거나 그 밖에 처분된 재산관계에서 발생하는 소득의 내용별로 규분한다. <개정 2011. 7. 25, 2020. 12. 29, 2022. 12. 31, 2024. 12. 31.>  \\n‘법인세법’ 제50조제2항에 따라 신탁재산에 귀속되는 소득에 대하여 그 신탁의 수탁자가 법인세를 납부하는 신탁  \\n‘자본시장과 금융투자업에 관한 법률’ 제9조제18항제1호에 따른 투자신탁(제17조제1항제5호에 따른 집합투자기구를 포함한다)  \\n‘자본시장과 금융투자업에 관한 법률’ 제25조제1항에 따른 집합투자업자경영보험회사의 특별계정  \\n제7조제1항제3호에 따른 수증금의 발생된 신탁  \\n비거주자의 소득은 제119조에 따라 규분한다.  \\n[전문개정 2009. 12. 31.]\\n\\n제5조(과세기간)  \\n① 소득세의 과세기간은 1월 1일부터 12월 31일까지 1년으로 한다.  \\n거주자가 사망한 경우의 세제기한은 1월 1일부터 사망한 날까지로 한다.  \\n② 거주자와 주소 또는 거소를 국외로 이전(이하 \"출국\"이라 한다)하거나 비거주자가 되는 경우의 과세기간은 1월 1일부터 출국한 날짜로 한다.  \\n[전문개정 2009. 12. 31.]\\n\\n제6조(납세지)  \\n① 거주자의 소득세 납세지는 그 주소지로 한다. 다만, 주소지가 없는 경우에는 그 거소지로 한다.  \\n② 비거주자의 소득세 납세지는 제120조에 따른 국내사업장(이하 \"국내사업장\"이라 한다)의 소재지로 한다. 다만, 국내사업장이 없는 경우에는 주된 거소지의 소재지로 하고, 국내사업장이 없는 경우에는 국내법인소득을 발생하는 장소로 한다. <개정 2015. 1. 1.>  \\n③ 납세지의 불분명한 경우에는 대통령령으로 정하는 바에 따라 납세지를 결정한다.  \\n[전문개정 2009. 12. 31.]\\n\\n제7조(원천징수 등의 납세지)  \\n① 원천징수하는 소득세의 납세지는 다음 각 호에 따른다. <개정 2012. 1. 1, 2023. 12. 31.>  \\n1. 원천징수하는 자가 거주자일 경우: 그 거주자의 주된 사업장이며, 다만, 주된 사업장이 외의 사업장에서 원천징수할 경우에는 그 사업장의 소재지, 사업장이 없는 경우에는 거주자의 주소지 또는 거소지로 한다.  \\n2. 원천징수하는 자가 비거주자일 경우: 그 비거주자의 소재지. 다만, 주된 사업장이 있는 경우에는 그 국내사업장의 소재지, 국내사업장이 없는 경우에는 비거주자가 계류하는 장소로 한다.  \\n원천징수하는 자가 법인일 경우: 그 법인의 본점 또는 주사무소 소재지  \\n4. 원천징수하는 자가 개인일 경우로서 법인의 지점, 영업소, 그 밖이 사업자등록제에 따라 독립적으로 회계처리를 하는 경우: 제6조에도 불구하고 그 사업장의 소재지의 소재지의 원천징수 거소에 적용된다. 다만, 대통령령으로 정하는 바에 따라 납세지를 결정한다.  ', content_length=1524, page=3), Page(content='천지상세에 의한 난시체로 할 수 있다.\\n5. 제156조, 제156조의2부터 제156조의9까지 및 제156조의10에 의한 원전집수의무자 제1호부터 제4호까지의 규정에서 정하는 난시체를 가지지 아니한 경우, 대통령령으로 정하는 바에 따른 난시체의 제150조에 따라 정하는 소속세의 난시체는 그 난시체합의 소속체로 한다. [전문개정 2009. 12. 31.]\\n\\n제6조(상속 등 공동의 난시체) ① 거주자 또는 비거주자가 사망하여 그 상속인이 피상속인에 대한 손세의 난시체의 자가 될 경우 그 소속세의 난시체는 그 피상속인의 주소지인 바에 따라 그 관할 세무서에 난시체로 신고하는 장소로 한다. ② 비거주자가 난시체를 둔 경우 그 비거주자의 소속세 난시체는 국내세무서장이 소속세로부터 그 난시체리의 주소지 중 난시체관리인이 대통령령으로 정하는 바에 따라 그 관할 세무서에 난시체로 신고하는 장소로 한다. ③ 제1항 또는 제2항에 따른 신고가 있는 경우에는 그 마련된 장소를 거주자 또는 비거주자의 소속세 난시체로 한다. ④ 제1항이나 제2항에 따른 신고가 없는 경우의 거주자 또는 비거주자의 소속세 난시체는 제106조 및 제207조에 따른다. ⑤ 국내에 주소가 없는 공무원 등 대통령령으로 정하는 사람의 소속 세의 난시체는 대통령령으로 정하는 장소로 한다. [개정 2019. 12. 31.]\\n\\n제7조(난시체의 지정) ① 국세청장 또는 관할 지방세청장은 다음 각 호의 어느 하나에 해당하는 경우에는 제6조부터 제8조까지의 규정에 의하여 대통령령으로 정하는 바에 따라 난시체를 지정할 수 있다. 1. 사업소득이 있는 거주자가 사망한 자의 세비지로 난시체를 신청한 경우 2. 제1조 외의 거주자 또는 비거주자로서 제6조 제2항의 규정에 따른 난시체가 난시체로 지정상황으로 1호와 부합하지 않아 난시체를 지정하거나 같은 형 제1호의 신청이 있는 경우로서 사업장 소속세를 난시체로 지정하는 것이 세무처에서 부적절하다고 인정되는 경우에 신청함에도 난시체 지정을 하지 아니한 경우에는 국세청장 또는 관할 지방세정장이 노무소득이란 난시체이며 각별히 증명하여야 한다. ③ 제1항에 의하여 난시체의 지정 사유가 소멸한 경우 국세청장 또는 관할 지방세청장은 난시체의 지정을 취소하여야 한다. \\n④ 제1항에 따른 난시체의 지정이 취소된 경우에도 취소 전의 소속세에 관한 신고, 청구, 납부, 그 밖의 행위의 효력에 영향을 미치지 아니한다. [전문개정 2009. 12. 31.]\\n\\n제8조(난시체의 변경신고) 거주자나 비거주자는 제6조부터 제9조까지의 규정에 따라 난시체가 변경된 경우 변경된 날부터 15일 이내에 대통령령으로 정하는 바에 따라 그 변경 후의 난시체에 관한 세무서장에게 신고하여야 한다. [전문개정 2009. 12. 31.]\\n\\n제9조(과세 철회) 소속세는 제6조부터 제10조까지의 규정에 따른 난시체를 관할하는 세무서장 또는 지방국세청장이 과세한다. [전문개정 2009. 12. 31.]\\n\\n제2장 거주자의 종합소득 및 퇴직소득에 대한 난시체로 <개정 2009. 12. 31.>\\n제7절 비과세 <개정 2009. 12. 31.>', content_length=1516, page=4), Page(content='# 제12조(비과세소득)\\n\\n다음 각 호의 소득에 대해서는 소득세를 과세하지 아니한다. <개정 2010. 12. 27., 2011. 7. 25., 2011. 9. 15., 2012. 2. 1., 2013. 1. 1., 2013. 2. 22., 2014. 1. 1., 2014. 3. 18., 2014. 12. 23., 2015. 12. 15., 2016. 12. 20., 2018. 3. 20., 2018. 12. 31., 2019. 12. 10., 2019. 12. 31., 2020. 6. 9., 2020. 12. 29., 2022. 8. 12., 2022. 12. 31., 2023. 1. 8., 2023. 12. 31., 2024. 12. 31., 2025. 10. 1., 2025. 12. 23.>\\n\\n1. \"공공신탁법\"에 따른 공익신탁의 이익\\n   가. ㆍ 받은 자목 또는 각 목의 이익으로 발생하는 소득\\n   나. 1) 개인의 주택 소유하는 자의 주택임대소득(제29조에 따른 기준시가가 1억원을 초과하는 주택 및 국외에 소유하는 주택의 임대소득을 제외한다)도 해당 과세기간에 대통령령으로 정하는 총수입금액의 합계액이 2천만원 이하인 주택의 주택임대소득(2018년 12월 31일 이전에 끝나는 과세기간까지 발생하는 소득으로 한정한다).\\n   다. 대통령령으로 정하는 농어업소득\\n   라. 대통령령으로 정하는 전통주 제조소에서 발생하는 소득\\n   마. 주금리기준 5년 이상인 채무의 이자 또는 양도세로 발생하는 소득으로서 연 3천만원 이하의 금액이, 이 경우 주금리기준 및 채무의 계상 필요한 사항은 대통령령으로 정한다.\\n   바. 대통령령으로 정하는 주식재벌에서 발생하는 소득\\n\\n2. 대통령령으로 정하는 어려움 또는 양식어에서 발생하는 소득\\n   가. 대통령령으로 정하는 복합 중인 변덕이 받은 급여\\n   나. 법령에 따라 동일한 사람이 동일 직장에 있는 급여\\n   다. \"산업재해보상법\"에 따라 수급권자가 받은 요양급여, 재활급여, 간병급여, 유족급여, 유족특별급여, 장해급여보장, 장해보장 또는 근로기준법에 의한 부상·질병 관련하여 근로자나 유족이 받은 배상·손실 또는 위탁(위탁대리)의 성질이 있는 급여\\n   라. \"근로기준법\" 또는 \"선원법\"에 따른 교차·전출 및 그 유족이 받는 요양급여, 휴업급여, 상병보상(傷病補償), 임신급여, 장애보상금, 육아휴직금, 장애인 유급방상, 소득세 유출장소, 장의비 장제비\\n   마. \"고용보험법\"에 따라 받는 실업급여, 육아휴직금, 웨이트리스 및 탁주근로자 단체 급여, 출산전후의 관계에 관하여 지연된 금액 방법, 이때 받는 전직지원금, \\'구직자모든\\'에 관한 공무원법에 따른 공무원법을 적용받는 사람이 받을 법정에 따라 받는 유욕학자적들(사학법인, 제307조에 따라 임명된 사무직 및 \\'사랑법\\'의 1986년 제205조에 따라 직위로는 또는 사학의 소속강주가 정하는 자에게 기울 고earing다 하는 급여에 해당하는 것 포함)\\n\\n3. \"국민연금법\"에 따라 받는 월 할인의 각 사유의 사적입니다\\n   가. \"공무원 재해보상법\", \"군인 재해보상법\", \"군인 재해보상법\", \"사이드학교직의 연금법\" 또는 별정의재권제법에 따라 받는 공무상요양비·요양급여·장해연금·비극과장해연금·비적급 장애편의분의 ̇ 자 자적보험소득, 사망자소득·사망보험소득·퇴직유증임소득·유족급여감금·퇴직유족연금법·퇴직유족연금법·퇴직유족연금법·자유상유족급여·위탁직가 바른 급여\\n\\n4. 대통령령으로 정하는 사항\\n   다. 대통령령으로 정하는 시비비참(賦稅賦給) 등의 직원의 급여\\n   라. 외국부(外國趨) 지방자치단체의 연봉자들과 외국의 지방정부를 포함한다. 이하 같다)도 대통령령으로 정하는 직원가 아닌 이상은 사람으로 정하는 사람이다. 다만, 외국작가가 그 나라에서 근무하는 우리나라 공무원의 급여에 대해서 소득세를 과세하지 아니하는 경우에 해당한다.', content_length=1884, page=5), Page(content='가. 「국가유공자 등 예우 및 지원에 관한 법률」 또는 「재해보상법 대상자 지원에 관한 법률」에 따라 받는 보훈급\\n금: 학술보조금\\n나. 「근로기준법」 예외에 관한 법률,에 따라 받는 급금\\n다. 직업훈련을 수행하기 위해 외국에서 직무 종사 군인·군무원이 받는 급금\\n라. 종합적인 군인·군무원의 전사(전상으로 인한 상태를 포함한다. 이하 같다) 한 경우 그 전사한 날이 속하는 과세기간의 급\\n거. 국외 또는 북한교류협력법에 따라 북한지역에 근로를 제공함에 따른 대통령령으로 정하는 급여\\n너. 「국민건강보험법」, 「고용보험법」, 또는 「노인장기요양보험법」에 따라 국가, 지방자치단체 또는 사용자가 부담하는 보험료\\n더. 생산직 및 그 관련 직에 종사하는 근로자로서 급여 수준 및 직종 등을 고려하여 대통령령으로 정하는 근로자가 대통령령으로 정하는 영전금으로. 야근근로 또는 휴일근로를 하여 받는 급여\\n너. 근로자가 사내근로자 이외의 수상한 방법으로 지급받는 식사 기타 음식료 등 근로자(식사 기타 식솔을 포함하지 아니하는 자에 한정한다)가 받는 월 20만 원 이하의 식대\\n머. 근로자 또는 그 배우자의 출산이나 자녀의 보육과 관련하여 사용자로부터 지급받는 다음의 급\\n1) 근로자(사용자와 대통령령으로 정하는 특별관리인 있는 제외자를) 또는 그 배우자의 출산 관련하여 자녀의 출생일 이후 2년 이내에 사용자가 보조금을 지급하는 데에 따라 자녀에 결제 지급받는 급여(2021년 1월 1일부터 대통령령이 정하는 2024년 12월 31일 사이에 지급되는 급여 포함한다) 전역\\n2) 근로자 또는 배우자의 해당 자녀가 기질적 기준으로 6세 이하이 되는 날과 이 이전 기간을 말한다. 이하의 조 제199조에서 정하는 자녀의 보육과 관련하여 사용자로부터 지급받는 급여로 해당 자녀 1명이 월 20만 원의 급여\\n서. 「교육 관련 법」 제28조 제11항에 따른 장학금 등 대상자의 근로를 다가가 지불받는 장학금(고용보험법, 제202 STEM법 제202 제4까지의 규정에 따른 대상에 재학하는 대상학생에 한정한다)\\n어. 「발전전림법」 제2024 제3조에 따른 재정비물로 받는 보상이나 다음의 보상(이하「직무발명보상」이라 한다)으로서 대통령령으로 정하는 급\\n저. 대통령령으로 정하는 복리후생적 정리의 급\\n제1. 제2023 제1 제3 제6호에 따른 소득 중 다음의 요건을 모두 충족하는 소득으로 대통령령으로 정하는 급에 의한 \\n1) 입원된 중임원(이하 이 것, 제202조 및 제1절25에서의 \"입원\"이라 한다) 본인이 소비하는 것을 목적으로 지급받는 지원을 받거나 구입할 목적 또는 영입으로써 대통령령으로 재판하는 경우 외의 다른 것\\n2) 이하 재직 중 다음 욕의 제공과 관련하여 모든 임원직에 공동으로 적용되는 기준이 있을 것\\n4. 연금수급 특수 과목 이외 하나에 해당하는 소득\\n가. 「구민연금법」, 「공무원연금법」 또는 「국민재해보상법」, 「사립학교법」, 「교직원연금법」, 「법인Working법」 또는 「구민연금과 직연금의 연계에 관한 법령」(이하「정년 관련법」)으로 인해 받는 유족급·퇴직유족급·장애유족급·상위유족급·순위유족\\n', content_length=1512, page=6), Page(content='소득세법\\n\\n5. 기타소득 중 기타 목적으로 하느번의 소득\\n   가. 구규공공동 예외 및 지원에 관한 법률에 따른 \\'독립보상선생자 지원에 관한 법률\\'에 따라 받는 보조금\\n   나. 구가자보험법에 따라 받는 상금과 보증금\\n   다. \\'상봉법\\'에 따른 혼장관 관계하여 받는 부상(費用)이 그 밖에 대통령으로 정하는 상금과 부상\\n   라. 종업원이 등 대한의 과제념의 퇴직한 후에 업무자5 또는 산업학력단으로부터 지급받게 대학교 학생의 소속 대학에 설치된 산업학력단으로 받는 직무평가금으로서 대통령으로 정하는 금액, 단, 자발적으로 명송금을 지급한 사용자 또는 산업학력단과 대통령으로 정하는 특정과제에 있는 자가 받는 직무정보 상금은 제외한다.\\n   마. 구국프로로의 승환 및 대우에 관한 법률에 따라 구국프로가 받는 위로지원금과 각 밖의 품목\\n   바. 문헌화의 보존 및 활용에 관한 법률에 따라 국가기장물안소로 지정된 자산ㆍ공공품의 으로 발생하는 소득\\n   사. 성과ㆍ공동금융을 발급으로 또는 미술관련 양도에 대하여 발생하는 소득\\n   아. 제21조제7항제26에 따른 종교소득의 중 다음의 어느 하나에 해당하는 소득\\n      1) 동제령, 제22조에 따라 국가비계이터저장 기관이 지정하는 표준표준업률부에 따른 종교관련종사자(이하 \"종교관련종사자\")가 받는 대봉령으로 지정하는 식사 또는 식약\\n      2) 종교관련종사자가 받는 대봉령으로 지정하는 실비보상 적립의 기금액\\n      3) 종교관련종사자 또는 배우재 출산이 6세 이하에 해당하는 자가 기선의 기준으로 판단한다) 자녀의 보육과 관련하여 종교단체법에 따라 받는 금액으로서 해당 자녀 1명당 20만원 이내의 금액\\n      4) 종교관련종사자가 재정지원금으로 정하는 사람을 지급받는 이익\\n   자. 법령ㆍ조례에 따른 위원회 등의 보수를 받기 위한 위원(학습질문 및 예술의 위원회를 포함한다) 등 이 받는 수당\\n   [전문개정 2009. 12. 31.]\\n\\n제13조 삭제 <2009. 12. 31.>\\n\\n제2절 과세표준과 세액의 계산 <개정 2009. 12. 31.>\\n제1항 제어계획 등록 <개정 2009. 12. 31.>\\n\\n제14조(과세표준의 계산) ① 거주자와 종합소득 및 퇴직소득에 대한 과세표준을 각각 구분하여 계산한다.\\n   ② 종합소득에 대한 과세표준(이하 \"종합소득과세표준\"이라 한다)은 제16조, 제17조, 제18조, 제20조의3, 제21조, 제24조부터 제26조까지, 제27조의2 및 제47조의2에 따라 세율은 이자소득금액, 배당소득금액, 사업소득금액, 근로소득액, 연금소득액 및 기타소득의 합계액에서 (이하 \"종합소득금액\"이라 한다)에서 제50조, 제51조의3, 제52조의1 및 제52조2에 따른 (이하 \"종합소득금액\"이라 한다)을 적용한 금액에 한한다. <개정 2013. 1. 1., 2014. 1. 1.>\\n', content_length=1388, page=7), Page(content='③ 다음 각 호에 따른 소득의 금액은 종합소득과세표준을 계산할 때 합산하지 아니한다. <개정> 2010. 12. 27, 2011. 7. 14, 2013. 1. 1, 2014. 12. 23, 2015. 12. 15, 2017. 12. 19, 2018. 12. 31, 2019. 12. 31, 2020. 12. 29, 2023. 12. 31.\\n1. 「조세특례제한법」 또는 이 법 제12조의 다감 가세제도가 아닌 소득\\n2. 대통령이 정하는 임용고시자(이하 “임용고시자”라 한다)의 근로소득\\n3. 제12조제1항의 세엽에 따라 원천징수하는 이자소득 및 배당소득 및 제62조제1항에 따른 직장공제회 초관밥린소득\\n4. 법인은 보는 단체 외의 단체 중 수익을 구성원에게 배분하지 아니하는 단체로서 단체명을 표시하는 금융거래를 하는 단체가 “금융신용기관 및 비금융법인 관련 통칙」 제2조제1항 각 목의 하나에 해당하는 금융시당(이하 “금융회사등”이라 한다)으로부터 받는 이자소득 및 배당소득\\n5. 「조세특례제한법」에 따른 분리과세되는 소득\\n6. 제33호부터 제50호까지의 규정 외의 이자소득과 배당소득(제12조제1항제8호에 따른 배당소득은 제외한다)으로서 소득의 합계액이 2천만원(이하 “이자소득등의 종합과세기준금액”이라 한다) 이하이면 제12조에 따른 원천징수 소득\\n7. 해당 과세기간에 대통령으로 정하는 총수입금액의 합계액이 2천만원 이하여 자주 특화대문소득(이하 “분리과세 주특화대소득”이라 한다). 이 경우 주택대문소득의 산정에 필요한 사항은 대통령이 정한다.\\n8. 다음 각 목에 해당하는 기타소득(이하 “분리과세기타소득”이라 한다)\\n가. 제12조제1항제3호부터 제8호까지, 제10조제1항, 제22조 및 제26조에 따른 기타소득(다만 및 마목의 소득은 제외한다)으로서 같은 조 제정에 따른 기타소득금액이 300만원 이하이면 제12조제5항에 따른 원천징수(제12조제1항제5호는 이와 같지 아니하는 경우를 포함한다) 소득, 단, 해당 소득이 있는 거주자와 종합소득과세표준을 계산하는 경우 그 소득을 합산하는 경우 간 있다. \\n나. 제12조제1항제6호에 관한 임업에 관한 기타소득\\n다. 제12조제1항제2조 및 같은 조 제2항에 따른 기타소득\\n라. 제12조제1항제2조에 따른 기타소득 중 특히 고부가가치업, 제20조에 따른 복권당첨금 \\n마. 밖에서 제21조제1항의 면 타 기타소득 중 주로 유사한 소득으로서 대통령이 정하는 기타소득\\n바. 제20조제1항제2조 및 제33조의 관련되는 연금소득 등을 주 목 외에 해당하는 연금소득\\n사. 가목 및 나목의 연금소득의 합계액이 1천500만원 이하인 경우 그 연금소득\\n아. 제32조에 따른 이자소득의 종합과세기준금을 계산한 배당소득는 제17조제3각 외의 부별 단서에 따라 다시 하는 금액을 포함하지 아니한다.\\n자. 제39조제1항제2항부터 규정에 해당되는 소득 중 이자소득은 “분리과세이자소득”이라 하고, 배당소득은 “분리배당소득”이라 한다.\\n차. 퇴직소득에 대한 과세표준은 “퇴직소득과세표준”이라 한다) 이 법 제22조에 따른 퇴직소득세액을 적용한 금액으로 한다. \\n[전문개정 2009. 12. 31.]\\n[시행일: 2022. 1. 1.] <제114조제2항제2호>(제21조제1항제2조와 관련 부분에 한정한다)\\n\\n제135조(세액 계산의 순서) 거주자 등의 종합소득 및 퇴직소득에 대한 소득세는 이 법에 특별한 규정이 있는 경우를 제외하고는 다음 각 호에 따라 계산한다. <개정> 2012. 1. 1, 2014. 1. 1, 2019. 12. 31, 2022. 12. 31.\\n', content_length=1730, page=8), Page(content='1. 제14조에 따라 계산한 각 과세표준에 대해 제55조제1항에 따른 세용(이하 “기본세용”이라 한다)을 적용하여 제55조에 따른 종합소득 산출세액과 퇴직소득 산출세액을 각 각 계산한다.\\n2. 제56조에 따라 계산한 각 산출세액에 있어서 제60조, 제57조제2항, 제58조, 제59조 및 제59조의2부터 제59조의7까지의 규정에 따른 세액공제를 적용하여 종합소득 결정세액과 퇴직소득 결정세액을 각 각 계산한다. 이 경우 제56조에 따라 배당세액공제가 있을 때는 산출세액에서 배당세액공제를 검토할 것과 제62조제1항에 따라 금액을 비교하여 큰 금액에서 제60조, 제57조, 제58조, 제59조 및 제59조의2부터 제59조의7까지의 규정에 따른 세액공제를 한 금액을 세액으로 하고, 제59조제6항에 따른 감면되는 세액이 있을 때에는 이를 공제한 각 과세표준을 각 각 계산한다.\\n[전문개정 2009. 12. 31.]\\n\\n제2관 소득의 종류와 금액\\n<개정 2009. 12. 31.>\\n\\n제16조(이자소득) ① 이자소득은 해당 과세기간에 발생한 다음 각 호의 소득으로 한다. <개정 2010. 3. 22., 2012. 1. 1., 2016. 12. 20., 2020. 12. 29., 2024. 12. 31.>\\n1. 국가나 지방자치단체가 발행한 채권 또는 종기의 이자와 할인액\\n2. 내부법인이 발행한 채권 또는 종기의 이자와 할인액\\n2의2. 삭제 2024. 12. 31.\\n3. 국내에서 받는 예금(입금 - 부금 - 예탁금 및 유면채를 포함한다. 이하 같음)의 이자\\n4. <상업자측순행법>에 따른 신용예(信用料) 또는 신용부금으로 인한 이익\\n5. 외국법인의 국내자산 또는 국내영업소에서 발생한 채권 또는 종기의 이자와 할인액\\n6. 외국에서 받는 예금의 이자\\n7. 대출금에 관한 채권 또는 증권의 할매조건부 매매차익\\n8. 대출형량으로 정하는 채권 또는 증권의 보험차익. 단, 다음 각 목의 어느 하나에 해당하는 보험의 보험차익은 제외한다.\\n가. 최소한 보험료를 납입한 날부터 만기 또는 종전체계일까지의 기간이 10년 이상으로 정하는 것\\n나. 대출형량으로 정하는 건강을 갖춘 영업보험\\n다. 대출형량으로 정하는 직장공제기초한림금\\n12. 제1항부터 제11항까지의 요산소득으로 금지 사용에 따른 대가로서의 성격이 있는 것\\n13. 제1호부터 제12호까지의 규정 중 어느 하나에 해당하는 소득을 발생시키는 거래 또는 행위와 자본시장에서 금융투자에 관한 법률 제20조에 따라 파생상품(이하 “파생상품”이라 한다)과 대출형량으로 정하는 내에 따라 결합된 경우 해당 발생상품의 거래 또는 행위로부터의 이익\\n③ 제1항 각 호에 따른 이자소득의 범위에 관한 사항은 대출형량으로 정한다.\\n[전문개정 2009. 12. 31.]\\n\\n제17조(배당소득) 배당소득은 해당 과세기간에 발생한 다음 각 호의 소득으로 한다. <개정 2012. 1. 1., 2017. 12. 19., 2020. 12. 29., 2022. 12. 31., 2024. 12. 31.>\\n', content_length=1462, page=9), Page(content='1. 내국법인으로부터 받은 이익이나 임금금 또는 분배금  \\n2. 법인으로부터 받은 내자본분 배당금 또는 분배금  \\n2의2. ‘법인세법’ 제27조 제2항에 따라 내국법인으로부터 받은 신탁재산(이하 ‘법인과세 신탁재산’이라 한다)으로부터 받는 배당금 또는 분배금  \\n3. 외해배당  \\n4. ‘법인세법’에 따라 배당으로 처리된 금액  \\n5. 국내 또는 국외에서 받는 대출형으로 정하는 집합투자기구로부터의 이익  \\n5의2. 국내 또는 국외에서 받는 대출형으로 정하는 파생결합증권과 차입금으로부터의 이익  \\n5의3. 금융기관 외 재산의 신탁계약에 의한 수익을 표시한 수익증권으로서 대출형으로 정하는 수익권으로부터의 이익  \\n5의4. 자본시장과 금융투자업에 관한 법률 제4조제1항에 따른 투자계약증권으로서 대출형으로 정하는 투자계약증권으로부터의 이익  \\n6. 외국법인으로부터 받는 이익이나 임금금 배당금 또는 분배금  \\n7. ‘국제재조정에 관한 법률’ 제27조에 따라 배당을 받는 것으로 간주되는 금액  \\n8. 제30조에 따른 공동사업에서 발생한 소득의 중 관련조 제1항에 따른 사업공동사업자의 손익배분에 해당하는 금액  \\n9. 제1조, 제2조, 제2호, 제3호부터 제3조까지, 제5조와 제5호의4까지, 제6조 및 제6조의2에 따른 소득세의 수입배분 성격이 있는 것  \\n10. 제1조, 제2조, 제2호, 제3호부터 제3조까지, 제5조와 제5호의4까지 및 제6조 제9항까지 중 이하에 해당하는 소득을 발생시키는 거처 또는 행위와 발생소득을 정하는 바에 대한 결정된 경우  \\n→ 제1조제9항에 따라 제1제법령이란 다음 각 호의 금액을 함하며, 이익 또는 주식, 사원, 및 법인 출자자에게 배당을 보는 것. <가장 무역 2012. 1. 1. 2022. 12. 31.>  \\n1. 주식의 소각이나 자본의 감소에 의한 주주 취득하는 금액, 그 밖의 자본의 주익(賤) 또는 세입·탈세 출자자 감소를 받아 사원이 출자자 취득하는 금액, 그 밖의 자본의 주익이 되는 출자자가 주식 또는 출자자 받아 주다.  \\n2. 법인의 이익금 전부 또는 일부를 수익을 통해 저식으로 취득하는 주식 또는 출자자빈의 경우. 단, 다음 각 목의 이나 다치하는 금액은 자본에 전입하는 경우는 제외한다.  \\n가. ‘상법’ 제450조제1항에 따라 자본전증권으로서 대출형으로 정하는 것  \\n나. ‘자본재발변법’에 따른 재무재결법령에 따라 제1조제1조에 의한 일정의 재무자산을 상하는 금액은 제외한다.  \\n3. 해산한 법인(법인으로부터 받은 단체를 포함한다)의 주주·사원·출자자가 함해 주식ㆍ법인에 해소으로 인한 잔여자산의 분배를 받는 금액이나 법이 재산에 가까워 하지: 출자자 또는 자본을 취득하기 위하여 사용하는 금액을 포함하는 금액. 단, 내국법인의 조세변경하는 경우로서 다음 각 목의 이나 하나에 해당하는 경우는 제외한다.  \\n가. ‘상법’에 따라 조세변경하는 경우  \\n나. 특별법에 따라 설립법인이 이행해 특별법의 개정 또는 폐지에 따라 회사로 조세변경하는 경우  \\n다. 법인 세법에 따라 내국법인의 조세변경하는 경우로서 대출형으로 정하는 경우  \\n4. 합병으로 소화할 법인의 주주·사원 또는 출자자가 합한 후 존재하는 법인으로 설립 법인으로부터 그 법인으로서 귀속하는 주식 등을 법이 공유할 금액 또는 법인의 자산이익관계에 그 법인으로서 소환한 법인에 그 주주 또는 자본자산을 취득하기 위하여 사용할 금액을 고려하는 금액  \\n', content_length=1669, page=10), Page(content='6. 법인이 분할되는 경우 분할되는 법인(이하 “분할법인”이라 한다) 또는 소멸한 분할법인의 상대방 법인(이하 “존속법인”이라 한다)의 주주가 분할로 설립되는 법인 또는 분할법인의 상대방 법인으로부터 분할의 주식 가액의 금액, 그 밖에 재산 가액의 합계액(이하 “분할가액”이라 한다)이나 분할법인의 상대방 법인의 주식(존속법인이 존속하는 경우에는 소株로 감소된 주식의 한정담보를 측정하기 위해 사용한 금액을 초과하는 금액)\\n\\n③ 배당소득을 얻게 하는 과세기간의 총수입금액으로 본다. 다만, 제정제11조, 제정2조, 제정3조 및 제정소득에 따른 배당소득 등 다음 각 호의 어느 하나에 해당하는 배당금을 제외한 본(3)과 제정제10조에 따른 배당소득 중 대출방법으로 정하는 배당소득에 대해서는 해당 과세기간이 송금하여 그 배당소득의 100분의 10에 해당하는 금액을 따른 금액 \\n\\n1. 제137조제1에 따른 의제배당(법인의 소득이 법인세가 과세되지 아니한 배당으로서 자본으로 인한 경우로 한정함)\\n2. 제138조제3호에 따른 기주사주 또는 자기자본의 소득이의 자본진입으로 인한 의제배당\\n3. 제138조제5호에 따른 재판기재자자의 자본진입으로 인한 의제배당\\n4. 제139조제1에 따른 의제배당\\n5. “조세특례제한법” 제132조에 따른 최저세액(最低限額)에 적용되는 법인세의 비과세·면제·감면 또는 소득공제를 적용한 법인으로부터 배당소득이 있는 경우에는 그 배당소득의 금액에 대한 경정은 정하는 것을 포함하여 산출금 \\n\\n6. “자본이전세법” 제203조와 관련하여 이 조 제138조제2호에 따른 재판기재자금을 감대하여 받은 배당\\n7. “법인세법”, 제108조의8 및 그 문서에 해당하는 자본전환을 감대하여 받은 배당\\n8. 제정제1조, 제정2조 및 제정3조를 적용하면 종료하여 해당 주식이 재산의 지적독점으로서의 이익을 발생하는 경우에는 그 주식의 출처지비율의 연구(예외적으로 사실상)이 해당 주식에 당해 해당 주식을 발생하는 법인의 자본금 방역주식에서 나누어 계산한 금액을 말한다. 이하 간주) 또는 출자금으로 계좌의 주식도 그 자본에 의하여 사용하는 목적\\n-\\xa0제137조 2012. 1. 1, 2017. 12. 19., 2020. 12. 29, 2022. 12. 31, 2024. 12. 31.\\n\\n8. 제138조를 적용할 때 주식, 출자계약 및 법인 재산의 재산가액과 함께 주식, 출자자 및 법인 재산을 취득하기 위하여 사용하는 금액에 기생이 필요한 사항을 대출방법으로 정한다. -\\xa0제138조 2024. 12. 31.\\n9. 제138조 각 호에 따른 배당소득금에 따른 배당소득금의에 관한 필요한 사항은 대출방법으로 정한다. [전문개정 2009. 12. 31.]\\n\\n제17조(배당소득) 배당소득은 해당 과세기간에 발생한 다음 각 호의 소득으로 한다. -\\xa0제139 2012. 1. 1, 2017. 12. 19., 2020. 12. 29, 2022. 12. 31, 2024. 12. 31.\\n1. 내국법인으로부터 받는 이익이 아닌 영업권의 배당 또는 분배금\\n2. 법인으로부터 국내법인으로 받는 배당금 또는 분배금\\n2의1. “법인세법”, 제105조제2항에 따라 내국법인으로 보는 신탁재산(이하 “법인이라 신탁재산”이라 한다)으로부터 받는 배당금 또는 분배금\\n3. 의제배당(擬制配當)\\n4. “법인세법”에 따라 배당소득으로 처분된 금액\\n5. 국내 또는 국외에서 받는 대출방법으로 정하는 집합투자기구로부터의 이익\\n5의2. 국내 또는 국외에서 받는 대출방법으로 정하는 파생결합증권 또는 파생결합사채로부터의 이익\\n5의3. 금전의 아닌 재산의 신탁재산에 관한 수익권의 표시를 수익권으로서 대출방법으로 정하는 수익권으로부터의 이익\\n5의4. 자본증권과 금융투자업에 관한 법률, 제42조제1항에 따른 투자자격증으로서 대출방법으로 정하는 투자 및 그 밖에', content_length=1856, page=11), Page(content=\"6. 외국법인으로부터 받는 이익이나 임대료 배당 또는 분배금  \\n7. 국외재차제조공에 관한 법률 제72조(이하 배당하는 것으로 간주될 금액  \\n제63조에 따른 공동사업체에 발생한 소득에 중과 조 제1항에 따른 합자조합사업자의 손익분배비율에 해\\n하는 금액  \\n\\n9. 제10조, 제12조, 제13조부터 제50조까지, 제51조이하 제63호까지, 제6조 및 제74조에 따른 소득과 유무\\n한 소득으로 수익분배 성격이 있는 것  \\n\\n10. 제10조, 제12조, 제13조부터 제50조까지, 제51조이하 제63호까지 및 제64호까지 중 본 하\\n나에 해당하는 소득을 발생시키는 거래 또는 행위와 파생상품으로 정하는 바에 따라 결정된 경우 해\\n당 받상솨풍 조사 또는 행위재분배의 이익  \\n② 제10조제3항에 따른 외제법인의 다음 각 호의 금액을 말하며, 이를 해적 주주, 사원, 외 박의 출자자에 배당한\\n것으로 본다.<가재법 2012. 1. 1, 2024. 12. 31.>  \\n\\n1. 주식의 소유가 자본의 감소로 인하여 주주가 취득하는 금액, 그 밖의 재산의 가치(價値) 또는 티차˙탈퇴 출\\n자와 감소로 인하여 사외일이 출자자가 취득하는 금액, 그 밖의 재산의 가액의 주주ㆍ사원이나 출자자의 주식\\n또는 출자자들을 취득하기 위하여 사용하는 금액을 포함하는 금액  \\n2. 법인의 여름의 전부 또는 일부를 자본 또는 출자를 잃을 관한해 취득하는 주식 또는 출자자본의 가\\n라마. 다만, 다음 각 목의 하나에 해당하는 금액이 자본에 전입하는 경우는 제외한다.  \\n가. '상법 제459조제1항에 해당 자본존립금으로서 대출형종으로 정하는 것  \\n나. '자본재카기법'에 따른 재기부분껌(금은 법 제13조제1항에 따른 토지의 재판매자에게 상응하는 금액\\n은 제외한다)  \\n\\n3. 해당한 법인(일로 보나 단체로 포함하는)의 주주ㆍ사원ㆍ출자자 또는 구성원이 법인 의 실명으로 인한 잔\\n여자산의 분배를 취득하는 금액이 그 밖의 재산의 가액에 해당 주식ㆍ출자자 또는 자본을 취득하기 위하여\\n사용된 금액을 초과하는 금액. 다만, 내국법인의 조직변경하는 경우 각 목의 하나에 해당하는 경우\\n는 제외한다.  \\n가. '상법'에 따라 조직변경하는 경우  \\n나. 특별법에 따라 설립된 법인이 해당 특별법의 개정 또는 폐지에 따라 '상법'에 따른 회사로 조직변경하는 경\\n우  \\n다. 그 밖의 법인에 따라 내국법인이 조직변경하는 경우로서 대출형금으로 정하는 경우  \\n라. 합병으로 소멸한 법인의 주주ㆍ사원 또는 출자자가 합병 후 설립된 법인의 설립법인의 주주로부터\\n법인을 취득하는 주식 또는 출자자에게 가액 또는 그 밖의 재산의 가치에 합당되는 금액을 초과하는 금액  \\n\\n5. 법인이 자치규제를 하고 있는 상황에서 제20조에 따른 자본적립을 허용하되 법인의 의  \\n주주 등은 지분비율이 증가 분배가 지분비율에 상응하는 정도로 10% 이하의 가액  \\n6. 법인이 불법탈세 법인 분할법인 법인이라 하더라도 소멸한 법인별의 상대 법인의 주주의 분배 받은 법인\\n의 분할법인 및 변환법의 상대 법인으로부터 법인으로서 취득하는 주식의 가액과 관계, 그 밖의 재산\\n가액의 형태에 따라 '법인변경'이라는 단어가 소멸한 법인의 주식별의 재산의 가치가 존중하는 경우에는 소득으로 감소세를 인정받는 경우에 사용한 금액을 초과하는 금액  \\n③ 배당소득금액을 해석 과세기준으로 한다. 다만, 제11조 제12조, 제13조 및 제61조에 따른 배당소득 중 다음 각 호의 어느 하나에 해당하는 배당금을 제외하되(3개) 제13조 제1항에 따른 배당소득 100분의 11에 해당하는 금액을 제외한다.  \\n<가재법 2020. 12. 29, 2023. 12. 31, 2024. 12. 31, 2025. 12. 23.>  \\n2. 제73조제2조가목에 따른 자기자본 또는 자기출자자본의 소규모의 자본전입에 따른 외적배당\", content_length=1858, page=12), Page(content='3. 제징제조나목에 따른 토지 재평가차액의 자본전입으로 인한 의제배당\\n4. 제징제조에 따른 의제배당\\n5. [조세특례제한법] 제32조에 따른 최저환율(最低限栗)에 적용되는 법인의 이세의 비과세・면제・감면 또는 소득금액에 대한 비과세・면제・감면을 포함한다) 반한 법인 중 대법원령은 정하며 법인으로부터 받은 배당소득이 있는 경우에 그 배당소득의 금액을 대법령으로 정한다.\\n6. ‘자산세평가방법’, 제28조제1항을 위반하여 o조 제징제조나목에 따른 재평가차액금으로 갈취하여 받은 배당\\n   * 법인의 취득이 비과세나목 및 다목에 해당하는 자본전입금으로 갈취한 배당\\n7. 제징제조1조・제3조・제4조 및 제6조를 정할 때 주식 또는 자본자산을 취득하기 위하여 사용하는 금액의 불법적인 경우에는 그 주식 또는 출처의 매각(매우부담적)인 경우에도 해명 주식의 취득을 다시 해명 주식의 방해하는 법인의 자본을 발행발횡상수로 나누어 계산한 금액을 말한다. 이하 같다) 또는 출처금액을 그 주식 또는 출처법인의 취득에 사용한 금액으로 본다.<제731 2012. 1. 1, 2024. 12. 31.>\\n8. 제징제를 적용할 때 주식, 출처자산 및 기타 회사의 취득가액에 해당 주식, 출처자산 및 기타 재산을 취득하기 위하여 사용하는 금액의 계산 등 필요한 사항은 대법령으로 정한다.<가장 2024. 12. 31.>\\n9. 제7장 각 호에 따른 배당소득의 제정에 대한 배당소득의 범위에 관한 필요한 사항은 대법령으로 정한다.\\n[전문개정 2009. 12. 31.]\\n[시행일: 2027. 1. 1.] 제17조제3항\\n\\n제1장 상세<2009. 12. 31.>\\n\\n제3조(사업소득) 사업소득은 다음 각 호의 소득으로 한다. 단, 제21조제1항제2호에 따른 기타소득으로 원천징수세가 과세표준청구신고된 경우에는 고려하지 아니한다.<개정 2017. 12. 19, 2018. 12. 31, 2019. 12. 31.>\\n1. 농업(작물재배의 종목 및 기타 식작물의 재배를 제외한다. 이하 같다) 임업 및 어업에서 발생하는 소득\\n2. 광업에서 발생하는 소득\\n3. 제조업에서 발생하는 소득\\n4. 전기, 가스, 증기 및 공기조절업에서 발생하는 소득\\n5. 수도, 하수 및 부기물 처리, 원료 재생업에서 발생하는 소득\\n6. 건설업에서 발생하는 소득\\n7. 도매 및 소매업에서 발생하는 소득\\n8. 유통 및 창고업에서 발생하는 소득\\n9. 숙박 및 음식점업에서 발생하는 소득\\n10. 정보통신업에서 발생하는 소득\\n11. 금융 및 보험업에서 발생하는 소득\\n12. 부동산업에서 발생하는 소득. 단, 『공익사업을 위한 토지의 취득 및 보상에 관한 법률』 제20조에 따른 공익사업과 관련하여 지정된 지역・지상권(지타당된 공공용 설정보권리 포함한다)을 설정하거나 대체함으로써 발생하는 소득은 제외한다.\\n13. 연구, 과학 및 기술서비스업(대통령령으로 정하는 연구개발비를 제외한다)에서 발생하는 소득\\n14. 사업의 관리, 사업 지원 및 각 서비스업에서 발생하는 소득\\n15. 교육서비스업(대통령령으로 정하는 교육기관을 제외한다)에서 발생하는 소득\\n16. 개인정보 및 사회복지사업(대통령령으로 정하는 사회복지사업을 제외한다)에서 발생하는 소득\\n17. 예술, 스포츠 및 관련 서비스업에서 발생하는 소득\\n18. 협회 및 단체(대통령령으로 정하는 협회 및 단체는 제외한다), 수리 기타 개인서비스업에서 발생하는 소득\\n', content_length=1642, page=13), Page(content='19. 가구나 고용활동에서 발생하는 소득  \\n20. 제160조제1항에 따른 복식부기미수자 차량 및 온란구 등 대통령령으로 정하는 사업의 유형과 성격을 양호한 것으로 발생하는 소득. 다만, 제504조제1항제1호에 따른 양도소득세에 해당하는 경우는 제외한다.  \\n21. 제1호부터 제20까지의 규정에 따른 소득과 유사한 소득으로서 종류를 목적으로 자가 계산한 취에 계 속적·반복적으로 행하는 활동을 통해 얻는 소득  \\n사업소득으로는 해당 사업자계기와 총수입금액에서 이에 사용된 필요경비를 공제한 금액으로 하며, 필요경비의 총 수입금액을 초과하는 경우를 초과하는 금액을 “결손금”이라 한다.  \\n제1항 각 호에 해당하는 범위에 관하여는 비 별의 특별한 규정이 있는 경우 외에는 ‘통계법’ 제22조에 따라 국가데이터처리가 고시하는 한국표준산업분류에 따르며, 그 밖의 사업소득의 범위에 관한 필요한 사항은 대통령령으로 정한다.  \\n[전문개정 2009. 12. 31]\\n\\n제20조(근로소득) ① 근로소득은 해당 과세기간에 발생한 다음 각 호의 소득으로 한다. <개정 2016. 12. 20, 2024. 12. 31.>  \\n1. 근로를 제공함으로써 받는 봉급·금로·보수·수당·임금·상여·수임과 관련하여 유사한 성격의 금여  \\n법인의 주주주혈하 또는 회사에 속하는 이익분립과의 결의에 따라 상여로 받는 소득  \\n‘법인세법’에 따라 성립된 요역  \\n2. 퇴직급여로 받는 소득으로 퇴직소득에 속하지 아니하는 소득  \\n3. 직원이거나 또는 대학의 교직원이 지급받는 직무발명보상금(제21조제1항제22호의2)에 따른 직무발명보상금은 제 외한다.  \\n4. 사업자와 법인의 성격·공통하는 재화 또는 용역을 사업자나 ‘도시개발 및 정정거래에 관한 법률’의 경계회사를 포함한다는 사업성이 존재하는 임금의 대통렴형으로 정하는 바에 따라 다르고 낮은 가격으로 제공하기 구혜받을 수 있도록 재입환으로써 해당 임금이 되는 이의  \\n② 근로소득의 제1항 각 호의 소득의 금액의 형태에는 비례소득의 금액을 제정하여, 이하 “충전액”이라 한다.  \\n이 경우 근로소득의 법에 관하여 필요한 사항은 대통령령으로 정한다.  \\n[전문개정 2009. 12. 31]\\n\\n제20조의2 삭제 <2006. 12. 30.>  \\n\\n제20조의3(연금소득) ① 연금소득은 해당 과세기간에 발생한 다음 각 호의 소득으로 한다. <개정 2013. 1. 1, 2014. 1. 1, 2014. 12. 23, 2023. 12. 31.>  \\n1. 공적연금에 관한 법에 따라 받는 가정 금융(“공적연금소득”이라 한다)  \\n2. 다음 각 목에 해당하는 금액으로 소득의 성격에 불구하고 연금제된 연금의 명칭을 정하는 대통령령으로 정하는. 제자(아래 “연금계좌계약”이라 한다)를 포함하는 법에 따라 지급받은 연금에 해당하는 경우로 인하여 연금성 용으로 인정하는 경우의 이 연금금“이라 하면, 연금소득의 종류를 “연금소득이라 한다”는 경우의 이 금.  \\n가. 제146조제1항에 따른 법전집제도이나 퇴직소득  \\n나. 제59조의3의에 따른 새해직원들이 받는 연금과림 2차 금  \\n다. 연금계좌의 운영세척이에 대한 증지금 억  \\n라. 법에 연금지급에 대해 발생한 소득세가 이의(豈)로 소득으로서 대통령령으로 정하는 소득  \\n3. 제정에 따른 소득과 가축과 연금 형태로 받는 것으로서 대통령령으로 정하는 소득  \\n공적연금소득은 2002년 1월 1일 이후에 발생한 연금 기거 및 부담금(국가 또는 지방자치단체의 부담 금을 포함한다. 이하 각금이라 기초로 하거나 2002년 1월 1일 이후 근로소득으로 하여 받는 연금소득으로 한다.  \\n', content_length=1756, page=14), Page(content='다.<개정 2013. 1. 1.>\\n③ 연금소득금액은 제1항 각 호에 따른 소득의 금액에 합계액(제2항에 따른 연금소득에 제외되는 소득과 비과세 소득을 제외하며, 이하 “총괄금액”이라 한다)에 의하여 제21조에 따른 연금소득질을 적용한 금액으로 한다.\\n[전문개정 2009. 12. 31.]\\n\\n제21조(이자소득) ① 기타 소득은 ·배당소득· 사업소득· 근로소득· 연금소득· 퇴직소득 및 양도소득 외의 소득으로서 다음 각 호에 규정하는 것으로 한다. <개정 2009. 7. 31, 2010. 12. 27, 2012. 1. 1, 2013. 1. 1, 2014. 12. 23, 2015. 12. 15, 2016. 12. 20, 2017. 12. 19, 2018. 12. 31, 2019. 8. 27, 2020. 12. 29, 2022. 12. 31.>\\n1. 상근, 현상금, 보람소 또는 이 이에 준하는 금품\\n2. 북록, 경품, 고 박의 추천권에 당청자에게 받는 금품\\n3. ‘사행행위 등 규제 및 처벌법’에서 규정하는 행위(적법 또는 불법 여부는 고려하지 아니하며) 참여하여 얻은 수익금이 이외\\n4. ‘한방(가사법)’에 따른 수준체불료, ‘경품·경쟁법’에 따른 손수익표, ‘전통소수산경기’에 관한 법률에 따른 소득상격기구를 포함하여 ‘국민체육진흥법’에 따른 체육지원투표와의 매개자 또는 저작권별의 양도 되는 사항의 적법 또는 불법을 고려하지 아니한다.\\n5. 저자 또는 소믈사(著作權)·함선제작·방송사에 외의 자가 저작자 또는 저작임접권의 양도 되는 사용의 대가를 받는 금품\\n6. 다음 각 목의 자산 또는 권리의 양도 - 대가 또는 사항의 대가를 받는 금품\\n가. 영화필름\\n나. 라디오·텔레비전방송 롤트 또는 필름\\n다. 그 밖에 가급적 나오도록 제23조로의 대법령으로 정하는 것\\n7. 광업·어업진·영상신적·산업재산·산업상 비밀·영업권(대통령령으로 정하는 제한 임차권을 포함한다.), 타사서(書)와의 체계화에 따른 길리, 지하수제 개발·이용료, 그 밖에 유사한 자산이 나. 권리를 양도하기 대하여 그 대가를 받는 금품\\n8. 물품(유통제공을 포함한) 또는 장소를 일식적으로 대여하고 사용료로 받는 금품\\n9. ‘전자상거래 등에 의한 소비자 보호에 관한 법률’에 따라 중소상인매매계약을 하는 자를 통해서 물품 등 장소를 대여하여 대금융통제와 정하는 규모에 의하여 사용하는 후신으로 받는 금품\\n10. ‘공익사업을 위한 토지 동의 규 해 제외 및 관련한 제정에 따른 공익사업 관련 지역권·시장권(지하 또는 공공의 전용권리를 포함한다)을 설정하여 대여하고 대여에도 발생하는 소득\\n11. 계약의 위반 또는 해약으로 인하여 받는 소득으로서 다음 각 목의 어느 하나에 해당하는 것\\n가. 위약금\\n나. 배상금\\n12. 부당이득 반환 지급받은 자는\\n13. 소유자가 없는 물건의 점유로 소유권을 취득하는 자산\\n14. 거주자·비거주자 또는 법인의 대통장관으로 정하는 특수관계인에 의하여 그 특수관계인에 의하여 거주자·비거주자 또는 법인으로부터 받는 경제적 이익으로 본 금액· 배당 또는 증여로 보지 아니하는 금품\\n15. 문서·학술·음악 또는 사진에 관한 창작물(‘그물’도 진행과 관련 병법에 대한 신문 및 ‘자기 등 정기간행물의 진행을 포함한다)·에 의해 정기기관에 게재하는 실형 및 위반과 유리나의 창작 또는 그 지명을 외국으로 변역하여 국외하다는 것을 포함하면서 받는 소득으로서 다음 각 목의 어느 하나에 해.\\n', content_length=1664, page=15), Page(content='당하는 것  \\n가. 일반론  \\n나. 저작권사용료 인세(印稅)  \\n다. 미술 · 음악 또는 사진에 속하는 창작물에 대하여 받는 대가  \\n\\n16. 채권에 관한 압류 수수료  \\n18. 대출형량으로 정하는 소기의 농·소상공인 공제보험의 해지일시  \\n19. 다음 각 목에 어느 하나에 해당하는 인적응역(제15호부터 제17호까지의 규정)을 임의적으로 제정하고 받는 대가  \\n가. 고용관계 없이 다양한 강연을 하고 강연료 등 대가를 받는 용역  \\n나. 라디오·텔레비전방송 등을 통하여 해설·계급 또는 연기 등을 하고 보수 또는 유사한 성질의 대가를 받는 용역  \\n다. 변호사, 공인회계사, 세무사, 건축사, 변리사, 고 박해 전문 지식 또는 특별한 기능을 가진 자가 지식 등을 활용하여 부수 또는 별 대가를 받고 제공하는 용역  \\n라. 박해 고용관계 없이 수수 또는 이와 유사한 성질의 대가를 받는 제공하는 용역  \\n21. 제20조의2제1항제3호나목 및 다목의 규정에 고 소득의 성격에 불구하고 연금외수입한 소득  \\n22. 질적 전이 여부란 주식매수선택권 행사 후 행사하지 고용관계 없이 주식매수선택권을 부여받아 임을 행상물로에 대한 의미  \\n22-2. 공무원 또는 박해의 교직을 퇴직한 후에 지급받는 직무발명보상금  \\n23. 뇌물  \\n24. 압살수택 및 배임죄에 의하여 받는 금품  \\n5. 제정<2020. 12. 29.>  \\n\\n26. 종합조세종합자문가 종합소득을 진행하는 종합관계종합자사로서의 활동과 관련하여 대출형량으로 정하는 종합단체로부터 받는 소득(이하 “종합소득으로 한다”)  \\n② 제1항 및 제19조제1항제21호에도 불구하고 대출형량으로 정하는 서화(書畫)·공동체의 양도로 발생하는 소득(사업장을 갖추는 등 대출형량으로 정하는 소득을 제외한다)은 기타소득으로 한다.  \\n\\n기타소득범위에 해당 과세기간의 총수입금에서 이에 사용된 필요경비를 공제한 금액으로 한다.  \\n29.  \\n\\n28. 제10조제26조에 따른 종합소득에 대하여 제20조제1항에 따른 근로소득으로 원천징수하기 위한 경우에는 해당 소득을 근로소득으로 본다.  \\n기타소득의 각 제정법에 박해에 관한 사항은 대출형량으로 정한다.  \\n\\n제21조(기타소득) 기타소득은 이자소득·배당소득·사업소득·근로소득·연금소득·퇴직소득 및 양도소득 외의 소득으로 다음 각 호에 대하여 규정하는 것으로 한다.  \\n2013. 1. 1, 2014. 12. 23, 2015. 12. 15, 2016. 12. 20, 2017. 12. 19, 2018. 12. 31, 2019. 8. 27, 2020. 12. 29, 2023. 7. 18, 2024. 12. 31.  \\n\\n1. 상금, 현상금, 보험금 또는 이에 준하는 금품  \\n2. 복권, 경품권, 및 각종 추첨에 당첨되어 받는 금품  \\n3. 사행행위 등 규제 및 처벌규정에 있어 규정하는 행위(적별 또는 불법 행위에 참여한가에 연은 재산상의 의미  ', content_length=1414, page=16), Page(content='', content_length=0, page=17), Page(content='', content_length=0, page=18), Page(content='', content_length=0, page=19), Page(content='', content_length=0, page=20), Page(content='', content_length=0, page=21), Page(content='', content_length=0, page=22), Page(content='', content_length=0, page=23), Page(content='', content_length=0, page=24), Page(content='', content_length=0, page=25), Page(content='', content_length=0, page=26), Page(content='', content_length=0, page=27), Page(content='', content_length=0, page=28), Page(content='', content_length=0, page=29), Page(content='', content_length=0, page=30), Page(content='', content_length=0, page=31), Page(content='', content_length=0, page=32), Page(content='', content_length=0, page=33), Page(content='', content_length=0, page=34), Page(content='', content_length=0, page=35), Page(content='', content_length=0, page=36), Page(content='', content_length=0, page=37), Page(content='', content_length=0, page=38), Page(content='', content_length=0, page=39), Page(content='', content_length=0, page=40), Page(content='', content_length=0, page=41), Page(content='', content_length=0, page=42), Page(content='', content_length=0, page=43), Page(content='', content_length=0, page=44), Page(content='', content_length=0, page=45), Page(content='', content_length=0, page=46), Page(content='', content_length=0, page=47), Page(content='', content_length=0, page=48), Page(content='', content_length=0, page=49), Page(content='', content_length=0, page=50), Page(content='', content_length=0, page=51), Page(content='', content_length=0, page=52), Page(content='', content_length=0, page=53), Page(content='', content_length=0, page=54), Page(content='', content_length=0, page=55), Page(content='', content_length=0, page=56), Page(content='', content_length=0, page=57), Page(content='', content_length=0, page=58), Page(content='', content_length=0, page=59), Page(content='', content_length=0, page=60), Page(content='', content_length=0, page=61), Page(content='', content_length=0, page=62), Page(content='', content_length=0, page=63), Page(content='', content_length=0, page=64), Page(content='', content_length=0, page=65), Page(content='', content_length=0, page=66), Page(content='', content_length=0, page=67), Page(content='', content_length=0, page=68), Page(content='', content_length=0, page=69), Page(content='', content_length=0, page=70), Page(content='', content_length=0, page=71), Page(content='', content_length=0, page=72), Page(content='', content_length=0, page=73), Page(content='', content_length=0, page=74), Page(content='', content_length=0, page=75), Page(content='', content_length=0, page=76), Page(content='', content_length=0, page=77), Page(content='', content_length=0, page=78), Page(content='', content_length=0, page=79), Page(content='', content_length=0, page=80), Page(content='', content_length=0, page=81), Page(content='', content_length=0, page=82), Page(content='', content_length=0, page=83), Page(content='', content_length=0, page=84), Page(content='', content_length=0, page=85), Page(content='', content_length=0, page=86), Page(content='', content_length=0, page=87), Page(content='', content_length=0, page=88), Page(content='', content_length=0, page=89), Page(content='', content_length=0, page=90), Page(content='', content_length=0, page=91), Page(content='', content_length=0, page=92), Page(content='', content_length=0, page=93), Page(content='', content_length=0, page=94), Page(content='', content_length=0, page=95), Page(content='', content_length=0, page=96), Page(content='', content_length=0, page=97), Page(content='', content_length=0, page=98), Page(content='', content_length=0, page=99), Page(content='', content_length=0, page=100), Page(content='', content_length=0, page=101), Page(content='', content_length=0, page=102), Page(content='', content_length=0, page=103), Page(content='', content_length=0, page=104), Page(content='', content_length=0, page=105), Page(content='', content_length=0, page=106), Page(content='', content_length=0, page=107), Page(content='', content_length=0, page=108), Page(content='', content_length=0, page=109), Page(content='', content_length=0, page=110), Page(content='', content_length=0, page=111), Page(content='', content_length=0, page=112), Page(content='', content_length=0, page=113), Page(content='', content_length=0, page=114), Page(content='', content_length=0, page=115), Page(content='', content_length=0, page=116), Page(content='', content_length=0, page=117), Page(content='', content_length=0, page=118), Page(content='', content_length=0, page=119), Page(content='', content_length=0, page=120), Page(content='', content_length=0, page=121), Page(content='', content_length=0, page=122), Page(content='', content_length=0, page=123), Page(content='', content_length=0, page=124), Page(content='', content_length=0, page=125), Page(content='', content_length=0, page=126), Page(content='', content_length=0, page=127), Page(content='', content_length=0, page=128), Page(content='', content_length=0, page=129), Page(content='', content_length=0, page=130), Page(content='', content_length=0, page=131), Page(content='', content_length=0, page=132), Page(content='', content_length=0, page=133), Page(content='', content_length=0, page=134), Page(content='', content_length=0, page=135), Page(content='', content_length=0, page=136), Page(content='', content_length=0, page=137)])\n"
     ]
    }
   ],
   "source": [
    "from pyzerox import zerox\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "### Model Setup (Use only Vision Models) Refer: https://docs.litellm.ai/docs/providers ###\n",
    "\n",
    "## placeholder for additional model kwargs which might be required for some models\n",
    "kwargs = {}\n",
    "\n",
    "## system prompt to use for the vision model\n",
    "custom_system_prompt = None\n",
    "\n",
    "# to override\n",
    "# custom_system_prompt = \"For the below pdf page, do something..somthing...\" ## example\n",
    "\n",
    "###################### Example for OpenAI ######################\n",
    "model = \"gpt-4o-mini\" ## openai model\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\" ## your-api-key\n",
    "\n",
    "\n",
    "# Define main async entrypoint\n",
    "async def main():\n",
    "    file_path = \"./income_tax.pdf\" ## local filepath and file URL supported\n",
    "\n",
    "    ## process only some pages or all\n",
    "    select_pages = None ## None for all, but could be int or list(int) page numbers (1 indexed)\n",
    "\n",
    "    output_dir = \"./documents\" ## directory to save the consolidated markdown file\n",
    "    result = await zerox(file_path=file_path, model=model, output_dir=output_dir,\n",
    "                        custom_system_prompt=custom_system_prompt,select_pages=select_pages, **kwargs)\n",
    "    return result\n",
    "\n",
    "\n",
    "# run the main function:\n",
    "result = asyncio.run(main())\n",
    "\n",
    "# print markdown result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "497f1da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q 'unstructured[md]' nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2853435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcacbfe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './documents/income_tax.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m markdown_path = \u001b[33m\"\u001b[39m\u001b[33m./documents/income_tax.md\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m loader = UnstructuredMarkdownLoader(markdown_path)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m document_list = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# data = loader.load()\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# assert len(data) == 1\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# assert isinstance(data[0], Document)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# readme_content = data[0].page_content\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# print(readme_content[:250])\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/langchain_core/document_loaders/base.py:85\u001b[39m, in \u001b[36mBaseLoader.load_and_split\u001b[39m\u001b[34m(self, text_splitter)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m     text_splitter_ = text_splitter\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text_splitter_.split_documents(docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/langchain_core/document_loaders/base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/langchain_community/document_loaders/markdown.py:96\u001b[39m, in \u001b[36mUnstructuredMarkdownLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_md\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/unstructured/partition/md.py:58\u001b[39m, in \u001b[36mpartition_md\u001b[39m\u001b[34m(filename, file, text, url, metadata_filename, metadata_last_modified, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m last_modified = get_last_modified_date(filename) \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     _, text = \u001b[43mread_txt_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     61\u001b[39m     _, text = read_txt_file(file=file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/unstructured/file_utils/encoding.py:133\u001b[39m, in \u001b[36mread_txt_file\u001b[39m\u001b[34m(filename, file, encoding)\u001b[39m\n\u001b[32m    131\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         formatted_encoding, file_text = \u001b[43mdetect_file_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file:\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m encoding:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/langgraph_my/.venv/lib/python3.13/site-packages/unstructured/file_utils/encoding.py:67\u001b[39m, in \u001b[36mdetect_file_encoding\u001b[39m\u001b[34m(filename, file)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_file_encoding\u001b[39m(\n\u001b[32m     63\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     64\u001b[39m     file: Optional[Union[\u001b[38;5;28mbytes\u001b[39m, IO[\u001b[38;5;28mbytes\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     65\u001b[39m ) -> Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     68\u001b[39m             byte_data = f.read()\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m file:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './documents/income_tax.md'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "markdown_path = \"./documents/income_tax.md\"\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "document_list = loader.load_and_split(text_splitter)\n",
    "# data = loader.load()\n",
    "# assert len(data) == 1\n",
    "# assert isinstance(data[0], Document)\n",
    "# readme_content = data[0].page_content\n",
    "# print(readme_content[:250])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
